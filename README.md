# ğŸ“š Novel Platform CDC íŒŒì´í”„ë¼ì¸ ìŠ¤í„°ë””

> **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ê¸°ìˆ ë“¤ì„ ë¯¸ë¦¬ ìŠµë“í•˜ê³  í…ŒìŠ¤íŠ¸ í•™ìŠµ**í•˜ê¸° ìœ„í•œ PostgreSQL â†’ Kafka â†’ Elasticsearch CDC(Change Data Capture) CDC íŒŒì´í”„ë¼ì¸ ì‹¤ìŠµ í”„ë¡œì íŠ¸

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš” 

ì´ í”„ë¡œì íŠ¸ëŠ” **ì‹¤ë¬´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  CDC ê¸°ìˆ ë“¤ì„ ì‚¬ì „ ê²€ì¦**í•˜ê¸° ìœ„í•œ í•™ìŠµ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. **Kafka Connect + Debezium**ì„ í™œìš©í•œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì—¬, ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ í•„ìš”í•œ ë‹¤ì–‘í•œ ì„¤ì •ë“¤ì„ ë¯¸ë¦¬ í…ŒìŠ¤íŠ¸í•˜ê³  ê¸°ìˆ ì„ ìŠµë“í–ˆìŠµë‹ˆë‹¤.

### ğŸ” **ê¸°ìˆ  ìŠµë“ ëª©ì **
- **ì‹¤ë¬´ ì ìš© ì „ ì‚¬ì „ ê²€ì¦**: ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ CDC íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹œ í•„ìš”í•œ ê¸°ìˆ ë“¤ì„ ë¯¸ë¦¬ í•™ìŠµ
- **ì„¤ì • ì˜µì…˜ í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ ì»¤ë„¥í„° ì„¤ì •ê³¼ Transform ì²´ì¸ì„ ì‹¤ì œë¡œ êµ¬í˜„í•˜ê³  ê²€ì¦
- **ì˜¤ë¥˜ í•´ê²° ê²½í—˜**: ì‹¤ì œ ìš´ì˜ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì„ ë¯¸ë¦¬ ê²½í—˜í•˜ê³  í•´ê²° ë°©ì•ˆ í•™ìŠµ
- **ì„±ëŠ¥ ìµœì í™” ì‹¤ìŠµ**: Production ë ˆë²¨ì˜ ì„¤ì •ë“¤ì„ ì§ì ‘ í…ŒìŠ¤íŠ¸í•˜ê³  ìµœì í™” ë°©ë²• ìŠµë“

### âœ¨ ì£¼ìš” ê¸°ëŠ¥
- **ì‹¤ì‹œê°„ CDC**: PostgreSQL WAL ê¸°ë°˜ ë³€ê²½ì‚¬í•­ ê°ì§€
- **ë™ì  ì¸ë±ìŠ¤**: ì‹œê°„ë³„ íˆìŠ¤í† ë¦¬ ì¸ë±ìŠ¤ ìë™ ìƒì„±
- **ë‹¤ì¤‘ ì‹±í¬**: ë©”ì¸/íˆìŠ¤í† ë¦¬/í´ë¦° ë°ì´í„° ë¶„ì‚° ì €ì¥
- **ë©”íƒ€ë°ì´í„° ì •ë¦¬**: Debezium wrapper ì œê±°
- **ì¤‘ë³µ ë°©ì§€**: Primary Key ê¸°ë°˜ Document ID

### ğŸ§ª **ìŠ¤í„°ë”” í•˜ì´ë¼ì´íŠ¸**
- **4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì»¤ë„¥í„° ì„¤ì •**ìœ¼ë¡œ ë‹¤ì–‘í•œ ì˜µì…˜ í…ŒìŠ¤íŠ¸
- **Transform ì²´ì¸ ì„¤ê³„**ë¶€í„° **ì˜¤ë¥˜ ì²˜ë¦¬ ì „ëµ**ê¹Œì§€ ì‹¤ìŠµ
- **Production ë ˆë²¨ ì„¤ì •**ë“¤ì„ ì§ì ‘ êµ¬í˜„í•˜ê³  ê²€ì¦
- **ì‹¤ì œ ë°œìƒí•œ ì˜¤ë¥˜ë“¤**ê³¼ **í•´ê²° ê³¼ì •**ì„ í†µí•œ ì‹¤ë¬´ ê²½í—˜
- **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ì ìš©ì„ ìœ„í•œ ê¸°ìˆ  ìŠµë“** ë° **ì„¤ì • ê²€ì¦ ì™„ë£Œ**

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    A[PostgreSQL<br/>novels & ex_platform_novel] --> B[Debezium<br/>Source Connector]
    B --> C[Kafka Topics<br/>ex_platform.public.*]
    
    C --> D[elasticsearch-ex-platform-connector]
    C --> E[elasticsearch-history-connector]
    C --> F[elasticsearch-sink-connector]
    
    D --> G[ex_platform_novel<br/>ë©”ì¸ ì¸ë±ìŠ¤]
    E --> H[ex_platform_novel_history_YYYY-MM-DD-HH-MM<br/>ì‹œê°„ë³„ íˆìŠ¤í† ë¦¬]
    F --> I[ex_platform_novel_clean<br/>í´ë¦° ë°ì´í„°]
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style H fill:#fff3e0
    style I fill:#f3e5f5
```

### ğŸ¨ ë°ì´í„° í”Œë¡œìš°
```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Multiple Elasticsearch Sinks
     â†“           â†“         â†“              â†“
 ì†Œì„¤ ë°ì´í„°   CDC ê°ì§€  ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼   ë‹¤ì¤‘ ì¸ë±ìŠ¤ ì €ì¥
   â†“           â†“         â†“              â†“
 ID ê¸°ë°˜      WAL ë¡œê·¸   í† í”½ ë¶„ê¸°     â”Œâ”€ ë©”ì¸ ë°ì´í„°
 ì¤‘ë³µ ë°©ì§€    ì‹¤ì‹œê°„     ë³‘ë ¬ ì²˜ë¦¬     â”œâ”€ ì‹œê°„ë³„ íˆìŠ¤í† ë¦¬  
 íŠ¸ë¦¬ê±° ìë™   ìŠ¤ëƒ…ìƒ·     Transform    â””â”€ ë°±ì—…/ë¶„ì„ìš©
```

## ğŸ“¦ ê¸°ìˆ  ìŠ¤íƒ & êµ¬ì„±

| êµ¬ë¶„ | ê¸°ìˆ  | ë²„ì „ | í¬íŠ¸ | ì—­í•  |
|------|------|------|------|------|
| **Database** | PostgreSQL | 15 | 5432 | ë©”ì¸ ë°ì´í„° ì €ì¥ì†Œ |
| **CDC** | Debezium | 2.4 | - | PostgreSQL WAL ëª¨ë‹ˆí„°ë§ |
| **Message Broker** | Apache Kafka | 7.5.0 | 6092 | ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° |
| **Connect Platform** | Kafka Connect | 7.5.0 | 8083 | Source/Sink ì»¤ë„¥í„° ê´€ë¦¬ |
| **Search Engine** | Elasticsearch | 8.11.0 | 9200 | ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ |
| **Visualization** | Kibana | 8.11.0 | 5601 | ë°ì´í„° ì‹œê°í™” |
| **Monitoring** | Kafka UI | latest | 8080 | í† í”½ & ì»¤ë„¥í„° ëª¨ë‹ˆí„°ë§ |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ï¸âƒ£ í”„ë¡œì íŠ¸ í´ë¡  & ì‹¤í–‰
```bash
git clone <repository-url>
cd CDC-Kafka-elasticsearch-postgresql-study

# ì „ì²´ ìŠ¤íƒ ì‹¤í–‰
docker-compose up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (ëª¨ë“  ì„œë¹„ìŠ¤ê°€ healthy ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
docker-compose ps
```

### 2ï¸âƒ£ ì»¤ë„¥í„° ìë™ ì„¤ì •
```bash
# CDC íŒŒì´í”„ë¼ì¸ ì»¤ë„¥í„° ë“±ë¡
./setup-connectors.sh

# ì»¤ë„¥í„° ìƒíƒœ í™•ì¸
curl -s http://localhost:8083/connectors | jq .
```

### 3ï¸âƒ£ ì°¨ë³„í™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
```bash
# í•œêµ­ ì›¹ì†Œì„¤ + í•´ì™¸ í”Œë«í¼ ì†Œì„¤ ë°ì´í„° ì‚½ì…
./test-insert-novels.sh
```

## ğŸ“Š ë°ì´í„° ëª¨ë¸

### ğŸ“– `novels` í…Œì´ë¸” - í•œêµ­ ì›¹ì†Œì„¤
```sql
CREATE TABLE novels (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,           -- ë‚˜ í˜¼ìë§Œ ë ˆë²¨ì—…, ì „ì§€ì  ë…ì ì‹œì  ë“±
    author VARCHAR(200) NOT NULL,          -- ì¶”ê³µ, ì‹±ìˆ‘, ê¹€ë…ì ë“±
    platform VARCHAR(100) NOT NULL,       -- ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ, ì¹´ì¹´ì˜¤í˜ì´ì§€, ë¦¬ë””ë¶ìŠ¤
    url TEXT NOT NULL,                     -- https://series.naver.com/novel/kr_*
    description TEXT,                      -- í•œêµ­ì–´ ì„¤ëª…
    genre VARCHAR(100),                    -- íŒíƒ€ì§€, ë¬´í˜‘, íšŒê·€, í•™ì› ë“±
    status VARCHAR(50),                    -- ì—°ì¬ì¤‘, ì™„ê²°, íœ´ì¬
    total_chapters INTEGER DEFAULT 0,     -- 50-149 í™”
    view_count INTEGER DEFAULT 0,         -- 10K-1M ë²”ìœ„
    like_count INTEGER DEFAULT 0,         -- 1K-50K ë²”ìœ„
    rating DECIMAL(3,2),                  -- 4.0-6.9 ë²”ìœ„
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### ğŸŒ `ex_platform_novel` í…Œì´ë¸” - í•´ì™¸ í”Œë«í¼ ì†Œì„¤
```sql
CREATE TABLE ex_platform_novel (
    id SERIAL PRIMARY KEY,
    platform_id VARCHAR(100) NOT NULL,    -- EN-1000, EN-1001 ë“±
    platform_name VARCHAR(100) NOT NULL,  -- Amazon Kindle, Webnovel, Royal Road
    title VARCHAR(500) NOT NULL,          -- Harry Potter, Lord of the Rings ë“±
    author VARCHAR(200) NOT NULL,         -- J.K. Rowling, J.R.R. Tolkien ë“±
    url TEXT NOT NULL,                    -- https://amazon.com/kindle/novel/en_*
    description TEXT,                     -- ì˜ì–´ ì„¤ëª…
    genre VARCHAR(100),                   -- Fantasy, Epic Fantasy, Cyberpunk ë“±
    status VARCHAR(50),                   -- ì—°ì¬ì¤‘, ì™„ê²°, íœ´ì¬
    total_chapters INTEGER DEFAULT 0,    -- 100-399 í™”
    view_count BIGINT DEFAULT 0,          -- 100K-5M ë²”ìœ„ (ë” ë†’ìŒ)
    like_count INTEGER DEFAULT 0,        -- 5K-100K ë²”ìœ„ (ë” ë†’ìŒ)
    rating DECIMAL(3,2),                 -- 4.5-6.9 ë²”ìœ„ (ë” ë†’ìŒ)
    publication_date TIMESTAMP,           -- ë°œí–‰ì¼
    last_chapter_date TIMESTAMP,          -- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸
    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(platform_name, platform_id)    -- í”Œë«í¼ë³„ ì¤‘ë³µ ë°©ì§€
);
```

## ğŸ”Œ Kafka Connect ì»¤ë„¥í„° êµ¬ì„±

> **ğŸ“š ìŠ¤í„°ë”” ë…¸íŠ¸**: ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ë‹¤ì–‘í•œ ì»¤ë„¥í„° ì„¤ì • ì˜µì…˜ë“¤ì„ í…ŒìŠ¤íŠ¸**í•˜ë©° ì‹¤ìŠµí–ˆìŠµë‹ˆë‹¤. Transform ì²´ì¸, ì˜¤ë¥˜ ì²˜ë¦¬, ì„±ëŠ¥ ìµœì í™”, í‚¤ ì²˜ë¦¬ ì „ëµ ë“± **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ Production ë ˆë²¨ì˜ ì„¤ì •ë“¤**ì„ ì§ì ‘ êµ¬í˜„í•˜ê³  ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

### 1. **PostgreSQL Source Connector (Debezium)**
```json
{
  "name": "postgres-connector",
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "database.hostname": "postgres",
  "database.dbname": "novel_platform",
  "table.include.list": "public.novels,public.ex_platform_novel",
  "topic.prefix": "ex_platform",
  "transforms": "route",
  "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
  "transforms.route.regex": "ex_platform\\.public\\.(.*)",
  "transforms.route.replacement": "ex_platform.public.$1"
}
```

### 2. **Elasticsearch Sink Connectors**

#### A. ë©”ì¸ ë°ì´í„° ì»¤ë„¥í„°
```json
{
  "name": "elasticsearch-ex-platform-connector",
  "topics": "ex_platform.public.novels",
  "index": "ex_platform_novel",
  "transforms": "unwrap,extractKeyField",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.extractKeyField.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
  "transforms.extractKeyField.field": "id"
}
```

#### B. íˆìŠ¤í† ë¦¬ ì»¤ë„¥í„° (ì‹œê°„ë³„ ë¶„í• )
```json
{
  "name": "elasticsearch-history-connector", 
  "topics": "ex_platform.public.ex_platform_novel",
  "transforms": "unwrap,addTS,route",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.addTS.type": "org.apache.kafka.connect.transforms.InsertField$Value",
  "transforms.addTS.timestamp.field": "history_timestamp",
  "transforms.route.type": "org.apache.kafka.connect.transforms.TimestampRouter",
  "transforms.route.topic.format": "ex_platform_novel_history_${timestamp}",
  "transforms.route.timestamp.format": "yyyy-MM-dd-HH-mm",
  "flush.synchronously": "true"
}
```

#### C. í´ë¦° ë°ì´í„° ì»¤ë„¥í„°
```json
{
  "name": "elasticsearch-sink-connector",
  "topics": "ex_platform.public.ex_platform_novel", 
  "index": "ex_platform_novel_clean",
  "transforms": "unwrap,extractKeyField",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.extractKeyField.type": "org.apache.kafka.connect.transforms.ExtractField$Key"
}
```

## ğŸ§ª ì»¤ë„¥í„° ì„¤ì • í…ŒìŠ¤íŠ¸ ìŠ¤í„°ë””

### ğŸ“Š **í…ŒìŠ¤íŠ¸í•œ ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜ë“¤**

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì»¤ë„¥í„°**ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜ë“¤ì„ ì‹¤ìŠµí–ˆìŠµë‹ˆë‹¤:

| ì»¤ë„¥í„° | í…ŒìŠ¤íŠ¸ ëª©ì  | ì£¼ìš” ì„¤ì • | í•™ìŠµ í¬ì¸íŠ¸ |
|--------|-------------|----------|-------------|
| **postgres-connector** | WAL ê¸°ë°˜ CDC | `pgoutput`, `RegexRouter` | í† í”½ ì´ë¦„ ë³€í™˜, ìŠ¤ëƒ…ìƒ· ëª¨ë“œ |
| **elasticsearch-sink-connector** | ê¸°ë³¸ ë™ê¸°í™” | `ExtractField$Value` | ë‹¨ìˆœ after í•„ë“œ ì¶”ì¶œ |
| **elasticsearch-ex-platform-connector** | ê³ ê¸‰ ë™ê¸°í™” | `ExtractNewRecordState` + `ExtractField$Key` | Document ID ì»¤ìŠ¤í„°ë§ˆì´ì§• |
| **elasticsearch-history-connector** | ë™ì  ì¸ë±ìŠ¤ | `TimestampRouter` + `InsertField$Value` | ì‹œê°„ë³„ ì¸ë±ìŠ¤ ë¶„í•  |

### ğŸ”§ **Transform ì²´ì¸ ì‹¤ìŠµ**

#### A. ë‹¨ìˆœ Transform (novels ì¸ë±ìŠ¤)
```json
"transforms": "extractValue,route"
// ëª©ì : Debezium after í•„ë“œë§Œ ì¶”ì¶œ
```

#### B. ë³µí•© Transform (ex_platform_novel ì¸ë±ìŠ¤)
```json
"transforms": "unwrap,extractValue,extractKeyField,route"
// ëª©ì : wrapper ì œê±° + Document ID ì»¤ìŠ¤í„°ë§ˆì´ì§•
```

#### C. ê³ ê¸‰ Transform (history ì¸ë±ìŠ¤)
```json
"transforms": "unwrap,extractAfter,addTS,route"
// ëª©ì : íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ + ë™ì  ì¸ë±ìŠ¤ ìƒì„±
```

### ğŸ¯ **ì˜¤ë¥˜ ì²˜ë¦¬ ì „ëµ í…ŒìŠ¤íŠ¸**

```json
{
  "errors.tolerance": "all",              // ëª¨ë“  ì˜¤ë¥˜ í—ˆìš©
  "errors.log.enable": "true",            // ì˜¤ë¥˜ ë¡œê¹… í™œì„±í™”
  "errors.log.include.messages": "true",  // ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€
  "behavior.on.malformed.documents": "warn",  // ì˜ëª»ëœ ë¬¸ì„œ ì²˜ë¦¬
  "behavior.on.null.values": "ignore"     // NULL ê°’ ì²˜ë¦¬
}
```

### âš¡ **ì„±ëŠ¥ ìµœì í™” ì‹¤ìŠµ**

```json
{
  "batch.size": 100,                      // ë°°ì¹˜ í¬ê¸° ì¡°ì •
  "max.in.flight.requests": 1,            // ìˆœì„œ ë³´ì¥
  "flush.timeout.ms": 10000,              // ì‘ë‹µì„± í–¥ìƒ
  "flush.synchronously": "true"           // SMT í™œì„±í™” (ì¤‘ìš”!)
}
```

## ğŸ¯ í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„

### 1. **ë™ì  ì¸ë±ìŠ¤ ìƒì„± (ì‹œê°„ë³„)**
```bash
# TimestampRouter SMT ì„¤ì •
"transforms.route.topic.format": "ex_platform_novel_history_${timestamp}"
"transforms.route.timestamp.format": "yyyy-MM-dd-HH-mm"

# ê²°ê³¼: ex_platform_novel_history_2025-01-15-14-30
```

### 2. **Debezium Wrapper ì œê±°**
```json
// ë³€ê²½ ì „: Debezium ì›ë³¸ êµ¬ì¡°
{
  "op": "c",
  "before": null,
  "after": {
    "id": 1,
    "title": "Lord of the Rings",
    "author": "J.R.R. Tolkien"
  },
  "source": {...}
}

// ë³€ê²½ í›„: ExtractNewRecordState ì ìš©
{
  "id": 1,
  "title": "Lord of the Rings", 
  "author": "J.R.R. Tolkien",
  "history_timestamp": 1751717783199
}
```

### 3. **ì¤‘ë³µ ë°©ì§€ Document ID ì „ëµ**
```json
// Primary Key ê¸°ë°˜ Document ID
"key.ignore": false,
"transforms.extractKeyField.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
"transforms.extractKeyField.field": "id"

// ê²°ê³¼: PostgreSQLì˜ PKê°€ Elasticsearch Document IDë¡œ ì‚¬ìš©
```

## ğŸ§ª ì‹¤ìŠµ ê°€ì´ë“œ

### Step 1: ê¸°ë³¸ CDC í…ŒìŠ¤íŠ¸
```bash
# 1. í•œêµ­ ì›¹ì†Œì„¤ ì¶”ê°€
docker exec postgres-db psql -U postgres -d novel_platform -c "
INSERT INTO novels (title, author, platform, genre, status) 
VALUES ('ìƒˆë¡œìš´ íšŒê·€ë¬¼', 'ê¹€íšŒê·€', 'ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ', 'íšŒê·€', 'ì—°ì¬ì¤‘');
"

# 2. í•´ì™¸ í”Œë«í¼ ì†Œì„¤ ì¶”ê°€  
docker exec postgres-db psql -U postgres -d novel_platform -c "
INSERT INTO ex_platform_novel (platform_id, platform_name, title, author, genre, status) 
VALUES ('AMZ-12345', 'Amazon Kindle', 'New Fantasy Epic', 'Fantasy Author', 'Epic Fantasy', 'ì—°ì¬ì¤‘');
"
```

### Step 2: ì‹¤ì‹œê°„ CDC ëª¨ë‹ˆí„°ë§
```bash
# Kafka í† í”½ ë©”ì‹œì§€ í™•ì¸
docker exec kafka bash -c "
kafka-console-consumer --bootstrap-server localhost:29092 \
  --topic ex_platform.public.novels --from-beginning
"

# Kafka UIì—ì„œ í† í”½ í™•ì¸
# http://localhost:8080
```

### Step 3: Elasticsearch ì¸ë±ìŠ¤ í™•ì¸
```bash
# ë©”ì¸ ì¸ë±ìŠ¤ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
curl -s "http://localhost:9200/ex_platform_novel/_search?pretty"

# íˆìŠ¤í† ë¦¬ ì¸ë±ìŠ¤ (ì‹œê°„ë³„ ë¶„í• )
curl -s "http://localhost:9200/ex_platform_novel_history_*/_search?pretty"

# í´ë¦° ë°ì´í„° ì¸ë±ìŠ¤ (wrapper ì œê±°ë¨)
curl -s "http://localhost:9200/ex_platform_novel_clean/_search?pretty"
```

### Step 4: ë³€ê²½ ì´ë ¥ ì¶”ì  í…ŒìŠ¤íŠ¸
```bash
# 1. ì¡°íšŒìˆ˜ ì—…ë°ì´íŠ¸
docker exec postgres-db psql -U postgres -d novel_platform -c "
UPDATE ex_platform_novel 
SET view_count = view_count + 1000, rating = 4.9
WHERE platform_id = 'AMZ-12345';
"

# 2. ë©”ì¸ ì¸ë±ìŠ¤ í™•ì¸ (ìµœì‹  ìƒíƒœ)
curl -s "http://localhost:9200/ex_platform_novel/_doc/1" | jq .

# 3. íˆìŠ¤í† ë¦¬ ì¸ë±ìŠ¤ í™•ì¸ (ë³€ê²½ ì´ë ¥)
curl -s "http://localhost:9200/ex_platform_novel_history_*/_search" | jq .
```

## ğŸŒ ì ‘ì† ì •ë³´

| ì„œë¹„ìŠ¤ | URL | ìš©ë„ |
|--------|-----|------|
| **Kafka UI** | http://localhost:8080 | í† í”½, ì»¤ë„¥í„°, ë©”ì‹œì§€ ëª¨ë‹ˆí„°ë§ |
| **Kibana** | http://localhost:5601 | ë°ì´í„° ì‹œê°í™” & ê²€ìƒ‰ |
| **Elasticsearch** | http://localhost:9200 | REST API |
| **Kafka Connect REST** | http://localhost:8083 | ì»¤ë„¥í„° ê´€ë¦¬ API |

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì„¤ì •

### Kafka Connect ì„¤ì •
```properties
# ì²˜ë¦¬ëŸ‰ ìµœì í™”
batch.size=100
max.in.flight.requests=1
flush.timeout.ms=10000

# ì˜¤ë¥˜ í—ˆìš©
errors.tolerance=all
errors.log.enable=true
errors.log.include.messages=true

# íˆìŠ¤í† ë¦¬ ì»¤ë„¥í„° ì „ìš©
flush.synchronously=true  # SMT í™œì„±í™” í•„ìˆ˜
```

### Elasticsearch ë§¤í•‘
```json
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "refresh_interval": "1s"
  },
  "mappings": {
    "properties": {
      "title": {"type": "text", "analyzer": "standard"},
      "author": {"type": "keyword"},
      "genre": {"type": "keyword"},
      "view_count": {"type": "long"},
      "rating": {"type": "float"},
      "created_at": {"type": "date"},
      "history_timestamp": {"type": "date"}
    }
  }
}
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. **ì»¤ë„¥í„° ìƒíƒœ í™•ì¸**
```bash
# ëª¨ë“  ì»¤ë„¥í„° ìƒíƒœ
curl -s http://localhost:8083/connectors | jq .

# íŠ¹ì • ì»¤ë„¥í„° ìƒì„¸ ìƒíƒœ
curl -s http://localhost:8083/connectors/postgres-connector/status | jq .

# ì»¤ë„¥í„° ì¬ì‹œì‘
curl -X POST http://localhost:8083/connectors/postgres-connector/restart
```

### 2. **MAP is not supported as document id ì˜¤ë¥˜**
```json
// ë¬¸ì œ: ValueToKey transformì—ì„œ ë³µìˆ˜ í•„ë“œ ì‚¬ìš©
"transforms.setKey.fields": "id,timestamp"  // âŒ

// í•´ê²°: ExtractField$Key ì‚¬ìš©
"transforms.extractKeyField.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
"transforms.extractKeyField.field": "id"    // âœ…
```

### 3. **TimestampRouter ë¯¸ì‘ë™**
```json
// í•„ìˆ˜ ì„¤ì • ëˆ„ë½
"flush.synchronously": "true"  // âœ… ë°˜ë“œì‹œ í•„ìš”
```

### 4. **PostgreSQL WAL ì„¤ì •**
```sql
-- postgresql.conf
wal_level = logical
max_wal_senders = 4
max_replication_slots = 4

-- í™•ì¸
SELECT * FROM pg_replication_slots;
```

## ğŸ“š ê¸°ìˆ  ìŠµë“ ê²°ê³¼ & í•™ìŠµ ìë£Œ

### ğŸ¯ **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ì ìš©ì„ ìœ„í•´ ìŠµë“í•œ ê¸°ìˆ ë“¤**
- **Transform ì²´ì¸ ì„¤ê³„**: 4ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ Transform ì „ëµìœ¼ë¡œ ë™ì¼í•œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì²˜ë¦¬
- **ì˜¤ë¥˜ ì²˜ë¦¬ ì „ëµ**: Production í™˜ê²½ì—ì„œ ì¤‘ìš”í•œ `errors.tolerance=all` ë“± ì•ˆì •ì„± ì„¤ì •
- **ì„±ëŠ¥ ìµœì í™”**: `batch.size`, `flush.timeout.ms` ë“±ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ
- **í‚¤ ì²˜ë¦¬ ì „ëµ**: Document ID ì»¤ìŠ¤í„°ë§ˆì´ì§•ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€ ë° ì—…ë°ì´íŠ¸ ì²˜ë¦¬
- **ë™ì  ì¸ë±ìŠ¤**: `TimestampRouter`ë¡œ ì‹œê°„ë³„ ì¸ë±ìŠ¤ ìë™ ìƒì„±
- **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ ì„¤ì •ë“¤**: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²€ì¦ëœ ì„¤ì • í™•ë³´

### ğŸ“– **ì°¸ê³  ë¬¸ì„œ**
- [Debezium PostgreSQL Connector](https://debezium.io/documentation/reference/connectors/postgresql.html)
- [Kafka Connect Elasticsearch](https://docs.confluent.io/kafka-connectors/elasticsearch/current/overview.html)
- [Single Message Transforms](https://kafka.apache.org/documentation/#connect_transforms)
- [PostgreSQL Change Data Capture](https://www.postgresql.org/docs/current/logical-replication.html)

### ğŸ§ª **ì‹¤ìŠµìœ¼ë¡œ ê²€ì¦í•œ ì„¤ì •ë“¤**
- âœ… **MAP ì˜¤ë¥˜ í•´ê²°**: `ExtractField$Key` ì‚¬ìš©ìœ¼ë¡œ Document ID ë¬¸ì œ í•´ê²°
- âœ… **TimestampRouter í™œì„±í™”**: `flush.synchronously=true` í•„ìˆ˜ ì„¤ì • ë°œê²¬
- âœ… **ë‹¤ì¤‘ Sink ì „ëµ**: í•˜ë‚˜ì˜ ì†ŒìŠ¤ì—ì„œ 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì¸ë±ìŠ¤ ìƒì„±
- âœ… **Debezium Wrapper ì œê±°**: `ExtractNewRecordState`ë¡œ ê¹”ë”í•œ ë°ì´í„° êµ¬ì¡°
- âœ… **ì˜¤ë¥˜ í—ˆìš© ëª¨ë“œ**: ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ ìš´ì˜ì„ ìœ„í•œ í•„ìˆ˜ ì„¤ì •

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ì ìš© ê³„íš)

1. **ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ** êµ¬ì¶• (Kafka Streams + WebSocket)
2. **ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§** (Schema Registry + Avro)
3. **ë©€í‹° í´ëŸ¬ìŠ¤í„° ë³µì œ** (MirrorMaker 2.0)
4. **ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬** (KSQL/Kafka Streams)

### ğŸš€ **ì‹¤ë¬´ ì ìš© ì¤€ë¹„ ì™„ë£Œ**
- âœ… **CDC íŒŒì´í”„ë¼ì¸ ì„¤ì • ê²€ì¦** ì™„ë£Œ
- âœ… **ë‹¤ì–‘í•œ ì»¤ë„¥í„° ì˜µì…˜ í…ŒìŠ¤íŠ¸** ì™„ë£Œ
- âœ… **ì˜¤ë¥˜ í•´ê²° ê²½í—˜** ì¶•ì 
- âœ… **ì„±ëŠ¥ ìµœì í™” ë°©ë²•** ìŠµë“
- âœ… **Production ë ˆë²¨ ì„¤ì •** í™•ë³´

---

**ğŸ“Œ ê¸°ìˆ  ìŠµë“ ìŠ¤í„°ë”” í”„ë¡œì íŠ¸**: ì´ í”„ë¡œì íŠ¸ëŠ” **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  CDC ê¸°ìˆ ë“¤ì„ ë¯¸ë¦¬ ìŠµë“í•˜ê³  ê²€ì¦**í•˜ê¸° ìœ„í•œ í•™ìŠµ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë³´ì•ˆ, ëª¨ë‹ˆí„°ë§, ë°±ì—… ë“± ì¶”ê°€ ê³ ë ¤ì‚¬í•­ì´ í•„ìš”í•©ë‹ˆë‹¤.